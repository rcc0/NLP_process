# NLP练习
TextCNN：对英文数据集进行二分类任务

BiLstim—CRF：对英文数据集进行序列标注任务

ESIM：对英文数据集实现文本蕴含任务

BERT_分类：实现BERT完成对英文数据集的二分类任务

word2vec：实现了对<<斗罗大陆>>进行word2vec，可以查询小说中人物关系等

基于知识图谱的电影问答系统：基于规则的问答系统，根据问题产生对应的查询语句，查询知识图谱后根据回答模版进行回复

词云：实现了中文词云、英文词云、不同形状的词云等

机器翻译：使用s2s+注意力机制完成中英机器翻译

文本摘要：使用s2s+指针网络完成中文文本摘要
### 项目结构描述
```
├── README.md                   // 描述文件
├── BERT_分类                   // 文本2分类
├── BiLstim—CRF                 // 序列标注
├── ESIM                        // 文本蕴含
├── TextCNN                     // 文本2分类
├── word2vec                    // 词向量
├── 词云                        // 构造词云
├── 基于知识图谱的电影问答系统  // 问答系统
├── images                      //存放仓库图片
├── 机器翻译                     //seq2seq的中英翻译
├── 文本摘要                    //seq2seq+指针网络的中文文本摘要
└── .gitignore
```



# NLP相关流程：
(个人总结，大致过程如下，可能不同人的总结不同)

## 英文文本：
### 1、分词：大多数情况下以空格进行分割
### 2、处理分词：往往缩略词还原、词性还原等（可以采用nltk库进行）
### 3、设计vocab、word2id、id2word：统计词频、排序等操作后设计vovab并建立id和word的想换转的字典
### 4、将分词结果转化为id值
### 5、截断或补全：取一个合理的长度，多则截断，少则补全（往往补0 <-> <unk>）

## 中文文本：
### 1、分词：比英文复杂一点，往往采用jieba分词等工具进行分词
### 2、处理分词：相对于英语该部分比较少
### 3、设计vocab、word2id、id2word：统计词频、排序等操作后设计vovab并建立id和word的想换转的字典
### 4、将分词结果转化为id值
### 5、截断或补全：取一个合理的长度，多则截断，少则补全（往往补0 <-> <unk>）
